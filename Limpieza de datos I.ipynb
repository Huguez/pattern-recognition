{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconocimiento de patrones: Preparación de datos\n",
    "### Ramón Soto C. [(rsotoc@moviquest.com)](mailto:rsotoc@moviquest.com/)\n",
    "![ ](images/blank.png)\n",
    "![agents](images/binary_data_under_a_magnifying.jpg)\n",
    "[ver en nbviewer](http://nbviewer.ipython.org/github/rsotoc/pattern-recognition/blob/master/Limpieza%20de%20datos%20I.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición\n",
    "\n",
    "Los datos son la base de la nueva economía de la información. Cada día se generan 2.5 x 10<sup>18</sup> bytes de datos ([aquí](http://www.vcloudnews.com/every-day-big-data-statistics-2-5-quintillion-bytes-of-data-created-daily/) un interesante *infographic* al respecto), provenientes de sensores, GPSs, redes sociales, mensajes electrónicos, transacciones comerciales, publicaciones regulares, etc. Esos datos permiten generar una gran cantidad de información para atender virtualmente cualquier problema. Sin embargo, antes de poder explotar la información contenida en ellos y antes de poder generar conocimiento de utilidad para la toma de decisiones, es necesario garantizar que los datos se encuentren en 'buenas condiciones'. \n",
    "\n",
    "Es una estimación bien conocida en tre los científicos de datos que el 80% del tiempo dedicado a la solución de un problema se invierte en la preparación de los datos: \n",
    "\n",
    "![](images/time.jpg)\n",
    "![ ](images/blank.png)\n",
    "\n",
    "El proceso de mejoramiento de los datos es lo que se denomina **preparación de los datos**. \n",
    "![](images/DataPreparation.png)\n",
    "![ ](images/blank.png)\n",
    "* La *limpieza de datos* consiste en rellenar valores faltantes, suavizar datos con ruido, identificar y remover valor atípicos y resolver inconsistencias. \n",
    "* La *integración de datos* es la integración de diversas fuentes de datos: bases de datos, cubos de datos o archivos. \n",
    "* La *selección de datos* consiste en seleccionar el conjunto de datos adecuado para analizar el sistema, incluyendo el muestreo. \n",
    "* La *selección de características* es un proceso mediante el cual se analizan las variables determinantes para describir los datos. \n",
    "* La *transformación de datos* incluye operaciones como normalización, agregación, codificación. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Limpieza de los datos \n",
    "\n",
    "### Valores faltantes \n",
    "\n",
    "El problema de valores faltantes es un problema muy frecuente al tratar de realizar cualquier tarea de análisis de datos y puede deberse a diversas razones: \n",
    "* Fallas en los mecanismos de medición (sensores defectuosos, por ejemplo) \n",
    "* Integración de conjuntos de datos no bien coordinados (mediciones con diferentes ciclos, por ejemplo) \n",
    "* Variables nuevas no consideradas o no disponibles originalmente \n",
    "* Respuestas omitidas intencionalmente por la fuente \n",
    "\n",
    "![](images/missingData.png)\n",
    "![ ](images/blank.png)\n",
    "\n",
    "La omisión de valores en el conjunto de datos puede tener diversos efectos y diferentes grados de impacto. En términos generales, se suelen considerar los siguientes grados de impacto, dependiendo del porcentaje de valores faltantes (*dumb rules*):\n",
    "La omisión de valores en el conjunto de datos puede tener diversos efectos y diferentes grados de impacto. En términos generales, se suelen considerar los siguientes grados de impacto, dependiendo del porcentaje de valores faltantes (*dumb rules*):\n",
    "* Menos de 1%: Trivial (no relevante)\n",
    "* 1-5%: Manejable\n",
    "* 5-15%: Manejable mediante métodos sofisticados\n",
    "* Más de 15%: Crítico, con impacto severo en cualquier tipo de interpretación\n",
    "\n",
    "Considérese el siguiente conjunto de datos tomados del conjunto de datos de diabetes:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 2] No such file or directory: 'Data sets/Pima Indian Data Set'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-7013e53a90a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Data sets/Pima Indian Data Set'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m: [Errno 2] No such file or directory: 'Data sets/Pima Indian Data Set'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Reconocimiento de patrones: Limpieza de datos\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.chdir('Data sets/Pima Indian Data Set') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              emb        gl2h         pad         ept        is2h         imc  \\\n",
      "count  768.000000  768.000000  768.000000  768.000000  768.000000  768.000000   \n",
      "mean     3.845052  120.894531   69.105469   20.536458   79.799479   31.992578   \n",
      "std      3.369578   31.972618   19.355807   15.952218  115.244002    7.884160   \n",
      "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
      "25%      1.000000   99.000000   62.000000    0.000000    0.000000   27.300000   \n",
      "50%      3.000000  117.000000   72.000000   23.000000   30.500000   32.000000   \n",
      "75%      6.000000  140.250000   80.000000   32.000000  127.250000   36.600000   \n",
      "max     17.000000  199.000000  122.000000   99.000000  846.000000   67.100000   \n",
      "\n",
      "              fpd        edad       class  \n",
      "count  768.000000  768.000000  768.000000  \n",
      "mean     0.471876   33.240885    0.348958  \n",
      "std      0.331329   11.760232    0.476951  \n",
      "min      0.078000   21.000000    0.000000  \n",
      "25%      0.243750   24.000000    0.000000  \n",
      "50%      0.372500   29.000000    0.000000  \n",
      "75%      0.626250   41.000000    1.000000  \n",
      "max      2.420000   81.000000    1.000000  \n",
      "\n",
      "     emb  gl2h  pad  ept  is2h   imc    fpd  edad  class\n",
      "0      6   148   72   35     0  33.6  0.627    50      1\n",
      "1      1    85   66   29     0  26.6  0.351    31      0\n",
      "2      8   183   64    0     0  23.3  0.672    32      1\n",
      "3      1    89   66   23    94  28.1  0.167    21      0\n",
      "4      0   137   40   35   168  43.1  2.288    33      1\n",
      "5      5   116   74    0     0  25.6  0.201    30      0\n",
      "6      3    78   50   32    88  31.0  0.248    26      1\n",
      "7     10   115    0    0     0  35.3  0.134    29      0\n",
      "8      2   197   70   45   543  30.5  0.158    53      1\n",
      "9      8   125   96    0     0   0.0  0.232    54      1\n",
      "10     4   110   92    0     0  37.6  0.191    30      0\n",
      "11    10   168   74    0     0  38.0  0.537    34      1\n",
      "12    10   139   80    0     0  27.1  1.441    57      0\n",
      "13     1   189   60   23   846  30.1  0.398    59      1\n",
      "14     5   166   72   19   175  25.8  0.587    51      1\n",
      "15     7   100    0    0     0  30.0  0.484    32      1\n",
      "16     0   118   84   47   230  45.8  0.551    31      1\n",
      "17     7   107   74    0     0  29.6  0.254    31      1\n",
      "18     1   103   30   38    83  43.3  0.183    33      0\n",
      "19     1   115   70   30    96  34.6  0.529    32      1\n",
      "20     3   126   88   41   235  39.3  0.704    27      0\n",
      "21     8    99   84    0     0  35.4  0.388    50      0\n",
      "22     7   196   90    0     0  39.8  0.451    41      1\n",
      "23     9   119   80   35     0  29.0  0.263    29      1\n",
      "24    11   143   94   33   146  36.6  0.254    51      1\n",
      "25    10   125   70   26   115  31.1  0.205    41      1\n",
      "26     7   147   76    0     0  39.4  0.257    43      1\n",
      "27     1    97   66   15   140  23.2  0.487    22      0\n",
      "28    13   145   82   19   110  22.2  0.245    57      0\n",
      "29     5   117   92    0     0  34.1  0.337    38      0\n",
      "..   ...   ...  ...  ...   ...   ...    ...   ...    ...\n",
      "738    2    99   60   17   160  36.6  0.453    21      0\n",
      "739    1   102   74    0     0  39.5  0.293    42      1\n",
      "740   11   120   80   37   150  42.3  0.785    48      1\n",
      "741    3   102   44   20    94  30.8  0.400    26      0\n",
      "742    1   109   58   18   116  28.5  0.219    22      0\n",
      "743    9   140   94    0     0  32.7  0.734    45      1\n",
      "744   13   153   88   37   140  40.6  1.174    39      0\n",
      "745   12   100   84   33   105  30.0  0.488    46      0\n",
      "746    1   147   94   41     0  49.3  0.358    27      1\n",
      "747    1    81   74   41    57  46.3  1.096    32      0\n",
      "748    3   187   70   22   200  36.4  0.408    36      1\n",
      "749    6   162   62    0     0  24.3  0.178    50      1\n",
      "750    4   136   70    0     0  31.2  1.182    22      1\n",
      "751    1   121   78   39    74  39.0  0.261    28      0\n",
      "752    3   108   62   24     0  26.0  0.223    25      0\n",
      "753    0   181   88   44   510  43.3  0.222    26      1\n",
      "754    8   154   78   32     0  32.4  0.443    45      1\n",
      "755    1   128   88   39   110  36.5  1.057    37      1\n",
      "756    7   137   90   41     0  32.0  0.391    39      0\n",
      "757    0   123   72    0     0  36.3  0.258    52      1\n",
      "758    1   106   76    0     0  37.5  0.197    26      0\n",
      "759    6   190   92    0     0  35.5  0.278    66      1\n",
      "760    2    88   58   26    16  28.4  0.766    22      0\n",
      "761    9   170   74   31     0  44.0  0.403    43      1\n",
      "762    9    89   62    0     0  22.5  0.142    33      0\n",
      "763   10   101   76   48   180  32.9  0.171    63      0\n",
      "764    2   122   70   27     0  36.8  0.340    27      0\n",
      "765    5   121   72   23   112  26.2  0.245    30      0\n",
      "766    1   126   60    0     0  30.1  0.349    47      1\n",
      "767    1    93   70   31     0  30.4  0.315    23      0\n",
      "\n",
      "[768 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"pima-indians-diabetes.data\", \n",
    "                 names = ['emb', 'gl2h', 'pad', 'ept', 'is2h', 'imc', 'fpd', 'edad', 'class'])\n",
    "\n",
    "print(df.describe())\n",
    "print \n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Como puede observarse, la variable *count* no es la misma para todas las columnas. Comparando con el despliegue de los datos, las diferencias en el valor de esta variable corresponde a los valores faltantes. Una mayor exploración podemos obtenerla de la siguiente manera:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print ('Tabla de valores nulos')\n",
    "print (df.isnull(), '\\n')\n",
    "\n",
    "print ('Contabilidad de valores nulos por columna')\n",
    "print (df.isnull().sum() )\n",
    "\n",
    "print ('Porcentaje de datos nulos en la columna *ept*')\n",
    "eptNullPje = df['ept'].isnull().sum() / df.shape[0] * 100\n",
    "print (eptNullPje)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como puede apreciarse, el porcentaje de valores faltantes en este segmento de datos (45% de valores faltantes) está muy por encima de lo que puede tratarse de manera directa, según las reglas anteriores. \n",
    "\n",
    "En muchos casos, incluso detectar los valores faltantes es un problema. En nuestros datos originales, lo valores faltantes vienen enmascarados como 0, no como un espacio vacío. En este caso, el procedimiento anterior fallaría pues no existen datos 'no disponibles'. Debemos primero analizar los datos y detectar cómo se manifiestan los valores faltantes. En nuestro ejemplo, asumimos que *ept*, esto es, el 'Espesor de la piel del tríceps' no puede tener un valor de 0 y, por lo tanto, ese valor representa un valor faltante. Para resolver el problema, debemos preparar los datos asignando una etiqueta *NaN* a los valores que consideramos como valores'faltantes': \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv(\"pima-indians-diabetes.data\", \n",
    "                 names = ['emb', 'gl2h', 'pad', 'ept', 'is2h', 'imc', 'fpd', 'edad', 'class'])\n",
    "\n",
    "print(df2)\n",
    "print\n",
    "\n",
    "df2.loc[df2['ept'] == 0,'ept'] = np.nan\n",
    "\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "En plataformas para *data science*, como R y Pandas, los valores marcados como 'NaN' suelen ser ignorados en las operaciones: \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print( df2.info(), '\\n')\n",
    "\n",
    "print( df2.describe(), '\\n')\n",
    "\n",
    "print('Suma y promedio de ept: ({}, {})'.format(df2['ept'].sum(), df2['ept'].mean()), '\\n')\n",
    "\n",
    "print('Promedio tomando en cuenta los 0s:', df2['ept'].sum()/768 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Tratamiento de valores faltantes \n",
    "\n",
    "El método más simple para tratar con el problema de valores faltantes es la *eliminación de casos*, también conocido como *análisis de casos completos. Este método está disponible en todos los paquetes de análisis de datos y es la opción por omisión en la mayoría. \n",
    "\n",
    "En Pandas podemos eliminar los valores faltantes de diferentes maneras. *DataFrame.dropna*() elimina todos los renglones en el DataFrame en los que hay al menos un valor *NaN*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df)\n",
    "print\n",
    "print(df.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El parámetro *tresh* en DataFrame.dropna() permite eliminar todos los renglones que no contengan al menos el número de columnas \"limpias\" expresado por el prámetro. En los ejemplos a continuación, se conservan 1) sólo los renglones que tienen al menos 8 columnas *limpias* y 2) los renglones que tienen al menos 7 columnas con valores definidos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.dropna(thresh=8), '\\n')\n",
    "print(df.dropna(thresh=7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputación \n",
    "\n",
    "El análisis de casos completos es una opción aceptable si el porcentaje de valores faltantes es pequeño. En la mayoría de los casos, es preferible reemplazar los valores faltantes por valores calculados por omisión o valores calculados. Esta operación se denomina **imputación**. En el siguiente ejemplo, todos los valores *NaN* son reemplazados por 0, lo cual pudiera seguir la lógica de que \"si el dato no está disponible es que en realidad era cero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(df)\n",
    "#print (df.describe())\n",
    "\n",
    "\n",
    "print\n",
    "print \"imputacion por media\"\n",
    "dfx = df.fillna(df.mean() )\n",
    "print(dfx)\n",
    "print\n",
    "print (dfx.describe())\n",
    "\n",
    "\n",
    "print\n",
    "print \"datos sin imputacion\"\n",
    "df3 = pd.read_csv(\"pima-indians-diabetes.data\", names = ['emb', 'gl2h', 'pad', 'ept', 'is2h', 'imc', 'fpd', 'edad', 'class'])    \n",
    "print(df3)\n",
    "print\n",
    "print df3.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sin embargo, en muchos casos un valor por omisión de cero no tiene sentido. En nuestro ejemplo con los datos de diabetes, un valor de cero en la columna *pad* (*Presión diastólica de la sangre*) es imposible en una persona viva. En este caso, una mejor opción es rellenar los valores faltantes por el mínimo registrado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df3 = df.fillna(df.min())\n",
    "print(df3, \"\\n\")\n",
    "print (df3.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otras alternativas comunes son rellenar los valores faltantes con el valor máximo o con valores estadísticos.\n",
    "\n",
    "Reemplazar los valores faltantes por el valor promedio de esa variable es uno de los métodos más comunes de imputación, sin embargo la media es una medida vulnerable a valores atípicos. Una alternativa más robusta ante este problema es la mediana. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print \"Rellenando con el valor máximo \\n\", df.fillna(df.max()).describe()\n",
    "print \"Rellenando con la media \\n\", df.fillna(df.mean()).describe()\n",
    "print \"Rellenando con la mediana \\n\", df.fillna(df.median()).describe()\n",
    "print \"Rellenando con la moda \\n\", df.fillna(df.mode()).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Otra alternativa común es rellenar los valores faltantes con el valor no nulo previo o el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print (df)\n",
    "\n",
    "print(\"Replicar hacia enfrente\\n\", df.fillna(method='pad'), \"\\n\")\n",
    "print(\"Replicar hacia atrás\\n\", df.fillna(method='bfill'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esta forma de tratar el problema de valores faltantes es muy común en análisis de series de tiempo. Esta estrategia suele designarse como *último valor conocido* y equivale a asumir que el sistema no pudo cambiar demasiado desde la última medición. En otros casos debe anaizarse si los datos realmente tienen una estructura local; esto es, determinar si tiene sentido esperar que los registros vecinos tengan valores cercanos. en el caso del conjunto de datos de diabetes, esta suposición no es válida.\n",
    "\n",
    "Podemos también limitar el número de registros que son modificados de esta forma:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(df.fillna(method='pad', limit=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolación\n",
    "La interpolación es un método formal para estimar valores en una serie de datos. La idea consiste en suponer que todos los puntos en la serie se encuentran sobre una curva subyacente, aunque desconocida. \n",
    "La forma más simple de interpolación es la *lineal*. En este caso se parte de dos puntos conocidos y los puntos intermedios (faltantes) se calculan como si estuvieran colocados sobre la línea recta que une a los puntos conocidos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'AxesSubplot' object does not support indexing",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-5dd5340cc1ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msharey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ept'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;31m#df['ept'].plot(ax=axes[0], grid=True, kind=\"bar\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'AxesSubplot' object does not support indexing"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True, figsize=(10,10))\n",
    "df['ept'].plot(ax=axes[0])\n",
    "df['ept'].plot(ax=axes[0], grid=True, kind=\"bar\")\n",
    "\n",
    "dfi = df['ept'].interpolate()\n",
    "dfi.plot(ax=axes[1])\n",
    "dfi.plot(ax=axes[1], grid=True, kind=\"bar\", color=\"green\")\n",
    "df['ept'].plot(ax=axes[1], grid=True, kind=\"bar\")\n",
    "\"\"\"\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=1, sharex=True, sharey=True, figsize=(50,50))\n",
    "#df['ept'].plot(ax=axes[0])\n",
    "#df['ept'].plot(ax=axes[0], grid=True, kind=\"bar\")\n",
    "\n",
    "\"\"\"\n",
    "dfx['ept'].plot(ax=axes[0])\n",
    "dfx['ept'].plot(ax=axes[0], grid=True, kind=\"bar\")\n",
    "plt.plot([1,2,3,4], [1,4,9,16], 'ro')\n",
    "plt.axis([0, 6, 0, 20])\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#print (df['ept'].interpolate())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La aproximación lineal, aunque es la más simple, es la menos natural. Es posible utilizar cualquier otro conjunto de curvas, típicamente de la forma, que se ajusten a los datos conocidos. A continuación se presentan ajustes a curvas cuadráticas y cúbicas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=1, sharex=True, sharey=True, figsize=(10,10))\n",
    "dfi = df['ept'].interpolate(method=\"quadratic\")\n",
    "dfi.plot(ax=axes[0])\n",
    "dfi.plot(ax=axes[0], grid=True, kind=\"bar\", color=\"red\")\n",
    "df['ept'].plot(ax=axes[0], grid=True, kind=\"bar\")\n",
    "\n",
    "dfi = df['ept'].interpolate(method=\"cubic\")\n",
    "dfi.plot(ax=axes[1])\n",
    "dfi.plot(ax=axes[1], grid=True, kind=\"bar\", color=\"red\")\n",
    "df['ept'].plot(ax=axes[1], grid=True, kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cualquier otra técnica de predicción puede ser empleada para rellenar los valores faltantes. Una de las más importantes es el razonamiento basado en casos ([RBC](https://en.wikipedia.org/wiki/Case-based_reasoning)). \n",
    "\n",
    "<hr style=\"border-width: 3px;\">\n",
    "\n",
    "### Tarea 2\n",
    "\n",
    "* Analice los problemas de valores faltantes en el conjunto de datos *Pima Indians Diabetes* completo. \n",
    "* Realice la imputación de los datos utilizando 3 aproximaciones diferentes y compare los resultados.\n",
    "* Realice una estimación de valores faltantes mediante interpolación.\n",
    "\n",
    "**Fecha de entrega**: Martes 29 de agosto.\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
